![alt text](https://i.imgur.com/qVQzihg.png)

# Data Description and Assessment

This dataset contains a list of video games with sales greater than 100,000 copies and only has data up to the year 2016. This dataset was found on Kaggle and the link is https://www.kaggle.com/gregorut/videogamesales. According to the description it was generated by scraping the data from the VGChartz website. This dataset contains 11 columns which are:

- Rank - Ranking of overall sales
- Name - The game's name
- Platform - Platform of the game's release
- Year - Year of the game's release
- Genre - Genre of the game
- Publisher - Publisher of the game
- NA_Sales - Sales in North America (in millions)
- EU_Sales - Sales in Europe (in millions)
- JP_Sales - Sales in Japan (in millions)
- Other_Sales - Sales in the rest of the world (in millions)
- Global_Sales - Total worldwide sales (in millions)

I was able to analyze this dataset to find information related to the video game industry such as finding the most popular/best-selling genres and platforms. I was also able to narrow down the data by region to get a closer look at regional differences in the video game industry. I also identified how publishers used this data to maximize their profits and how they focused solely on genres and platforms that were proven to be highly profitable.

### - Structure/Shape of the data:

Rectangular

Qualitative (categorical) - Rank, Name, Platform, Year, Genre, and Publisher

Quantitative (numerical) - NA_Sales, EU_Sales, JP_Sales, Other_Sales and Global_Sales

### - The granularity of the data:

A fine level of granularity since each data point represents a single video game.

### - Scope and completeness of the data:

The scope of the data is large since the dataset contains sales information for many different regions.

### - Temporality of the data:

The data is from 1980-2016 (There are only 3 items in the year 2017 so these will be removed).

### - Faithfulness of the data

This dataset was last updated in 2016 so there won't be any way to see how the trends have changed since the creation of this dataset. Some rows are missing data in the Year and Publisher columns and some contain duplicate information. There are also some errors with the years because the max value in the year column is 2020 which is incorrect since there can only be years from 1980-2016.

# A look at the dataset before any cleaning was done

![alt text](https://i.imgur.com/VwHxeXY.png)

![alt text](https://i.imgur.com/PpYsxiE.png)

# Cleaning the data

### 1. Load and check the data frame:

Loaded the data using pd.read_csv and created a data frame called vgsales. Then I used .describe() and .describe(include = "O") to check for any rows with missing values. I noticed that some rows had their Year and Publisher columns with missing values.

### 2. Remove NaN Values:

Used .isna() to check and remove NaN values from Year and Publisher columns and created a new data frame called vgsales2 without those NaN values.

### 3. Remove rows with invalid years:

Removed rows that had years after 2016 since there weren't enough rows to bother including data after 2016.

### 4. Remove duplicates:

Removed rows that had the same name, platform, and year.

### 5. Check datatypes:

Changed the datatype of Year from float64 to int64.

After cleaning the data we go from 16,598 rows to 16,286 rows.

# A look at a filtered dataset showing only rows with NaN values:

![alt text](https://i.imgur.com/1aJ50dM.png)

# Removing rows of games that were released after 2016:

![alt text](https://i.imgur.com/kHLhd7d.png)

# Single variable distribution plot

![alt text](https://i.imgur.com/7C86MNk.png)

From the visualization we can see the platform with the most games (with sales greater than 100,000 copies) is the DS and PS2. 

These are two platforms where games are much cheaper to make and are two platforms that had a much longer lifespan of games being released compared to other platforms.

![alt text](https://i.imgur.com/PXqpPCc.png)

From this visualization, we can see the top genres and how many games belong in that category. 

The action genre is number one and has more than 3000 games (with sales greater than 100,000 copies) in the dataset. This may be why so many publishers choose that genre when creating a game because it has proven to be successful in making a profit.

![alt text](https://i.imgur.com/VVWtvwJ.png)

This visualization shows the total amount of games each publisher has made (with sales greater than 100,000 copies). 

Electronic Arts is the publisher with the most games in this dataset and they tend to develop games yearly that are a part of the top 2 genres (action and sports) that have been proven to be successful at making a profit.

![alt text](https://i.imgur.com/IVeOYyC.png)

This visualization shows the count of games with sales greater than 100,000 copies over the years.

From this graph, we can see as the gaming industry grew more popular so did the profits. For example, we can see that in the '90s there were fewer than 500 games that sold more than 100,000 copies while in the 2010s there were over 2500 games that sold more than 100,000 copies.

# Multiple variable plots

![alt text](https://i.imgur.com/OwP3bjN.png)

This visualization shows sales by region over the years with North America having the most sales when compared to Europe, Japan, and Other regions.

![alt text](https://i.imgur.com/EwrpkdK.png)

This visualization shows the top 16 publishers' sorted by their global sales.

From the previous graph showing the publisher with the most games with sales greater than 100,000 copies we saw that Electronic Arts was number one but in terms of sales Nintendo has 1.7 billion global sales compared to Electronic Arts's 1 billion global sales.

# Model 1: Decision Tree

The first prediction model that was used was the Decision Tree model with max depths of 3, 10, and 20.

Dummy variables were created for the genre column. The X data frame contains only the year and genre columns. 

The Y data frame contains only the global sales column. The data was split into training and testing sets which was used to get the mean squared error for both training and testing for each max depth.

X data frame:
![alt text](https://i.imgur.com/nDj1Wfv.png)

Y data frame:
![alt text](https://i.imgur.com/NiitSMG.png)

The following are the mean-squared errors for each model:  

max_depth = 03  
MSE test: 0.882705934443137  
MSE train: 0.07185287613771285  

For the model of max depth 3 both the testing and training sets perform well since the MSE for both were relatively low.

The model fits the training data well, meaning that it can predict 'global sales' values with smaller errors on average, and while the training set's MSE is higher than the training set's MSE it is still low enough not to suffer from overfitting meaning that the model does well with unseen data.

max_depth = 10  
MSE test: 0.5633199009670071  
MSE train: 7.351989767353041e-07  

For the model of max depth 10, the testing set performs really well meaning that it can easily predict 'global sales' values.
But with the testing set its MSE is really high (meaning that the model suffers from overfitting) which means that the model will perform well on data that it has already been trained on but with unseen data, it does not perform well.

max_depth = 20  
MSE test: 0.5616361571516266  
MSE train: 7.7100464418941e-32  

For the model of max depth 20, the testing set performs really well just like the model with a max depth of 10, meaning that it can easily predict 'global sales' values. 
But like the model of a max depth of 10, the testing set's MSE is really high (meaning that the model suffers from overfitting) which means that the model will perform well on data that it has already been trained on but with unseen data, it does not perform well.

# Model 2: Linear Regression

![alt text](https://i.imgur.com/N91azEV.png)
![alt text](https://i.imgur.com/5VVu5b2.png)
![alt text](https://i.imgur.com/xXOztG4.png)

The second prediction model that was used was the Linear Regression model.

The independent variables for the three models were the publisher, year, and genre columns.  
The dependent variable was the global sales column.   

The model with the genre independent variable had a R-squared value of 0.012.  
The model with the year independent variable had a R-squared value of 0.006.   
The model with the publisher independent (which is a scaled version of the publisher column that only contains publishers who had more than 200 games in the dataset) variable had a R-squared value of 0.084 which of the 3 models was the closest to 1.

# Sales Distribution By Region and Genre Using Heatmap

![alt text](https://i.imgur.com/kzTKtDN.png)

This visualization shows the sales distribution for each region according to the genre of the game. 

According to this heat map for the North American, European, and Other regions, the best-selling genre is the Action genre. But for the Japanese region, the best-selling genre is the Role-Playing genre. 

Another thing I noticed is that the Strategy genre seems to be the genre with the least sales for all the regions. So if you were a publisher looking into making a video game and were located in North America the Action genre would be a very safe bet in terms of making a profit, while there would be some risks in making a game in the Strategy genre category.

# Sales Distribution By Genre Using Lineplot

![alt text](https://i.imgur.com/yCsd9Bi.png)

This visualization shows the global sales distribution by the genre of the game. 

From this visualization, we can see that the top three genres are Action, Sports, and Shooter. So no matter the region you choose to publish your game these three genres are very safe choices to pick that have a high chance of making a profit.
